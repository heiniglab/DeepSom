{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get variant image from a BAM file\n",
    "\n",
    "Here, we take a variant and construct a multichannel image from it to prepare Neural Network data.\n",
    "\n",
    "The variant position is taken from the VCF file and the image is constructed out of the reads exctracted from BAM file. No variant annotations from VCF file are encoded in the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we fetch reads using pysam library and pileup them around the variant.\n",
    "For each read the cigar string is analysed to place read bases correctly (taking into account clips, insertions,deletion).\n",
    "If and only if the variant is an insertion, we account for this by placing insertions in the pileup image. Insertions that are not at the variant site are not inserted in the image.\n",
    "However, we mark absolutely all inserions (even if they are not at the variant site) in a dedicated channel by placing '1' at the position after which the insertions start. We mark all insertions with only one symbol, regardles of the insertion length.\n",
    "For deletions, we fill a dedicated channel with '1' throughout the whole deletion length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pysam\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import sys\n",
    "%matplotlib inline\n",
    "\n",
    "from variant_to_image_helpers import * #some helper functions\n",
    "#from variant_to_image import * #some helper functions\n",
    "\n",
    "#reference FASTA file\n",
    "ref_fasta_file = '/storage/groups/epigenereg01/workspace/projects/vale/calling/MLL/resources_GRCh37/GRCh37.fa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example variant\n",
    "\n",
    "variant = {'pos': 9626706, 'chrom': '3', 'ref': 'G', 'alt': 'T', 'refpos': 9626706}\n",
    "bamfile = '/storage/groups/epigenereg01/workspace/projects/vale/data/icgc/GACA-CN/bam/6d938d97451c6a07ada759f376b95a4a.bam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_max_height = 20 #max image height, the probability to have a read depth above this value should be small\n",
    "\n",
    "image_width = 150 # image width: 2x the most probable read length\n",
    "\n",
    "image_crop_strategy = 'topbottom'\n",
    "\n",
    "image_sort_by_variant = True #sort reads by letter in the variant column\n",
    "\n",
    "image_check_variant_column = True #check if there's any alternative allele in the variant column (some variants may be corrupted due to a bad variant position etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_column_idx = image_width//2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference sequence: TTGTATAGTGATGAAGTCTGAGATTTTAGCACACCTGTCACCTAAGTAATGTACATTGTACCCAATATGTATTTTGTTTGTTTGTTTGTTTGTTTGTTTGTTTGTTTTGAGACAGAGTTTCGCTCTTGTTGCCCAGGCTGGAGTGCAATG\n"
     ]
    }
   ],
   "source": [
    "def get_ref_bases(variant):\n",
    "\n",
    "    '''\n",
    "    Get reference sequence around the variant positon, variant being roughly in the center\n",
    "    '''\n",
    "    reffile = pysam.FastaFile(ref_fasta_file)\n",
    "\n",
    "    ref_bases = reffile.fetch(variant['chrom'], variant['refpos']-variant_column_idx-1, variant['refpos']-variant_column_idx+image_width-1) \n",
    "    \n",
    "    ref_bases = ref_bases.upper()\n",
    "    \n",
    "    if ref_bases[variant_column_idx]!=variant['ref']:\n",
    "        raise Exception('Variant reference allele not found in the right position in the reference bases string!')\n",
    "\n",
    "    ref_bases = list(ref_bases)\n",
    "    \n",
    "    print('Reference sequence: ' + ''.join(ref_bases))\n",
    "    \n",
    "    ref_bases = np.array(list(map(encode_bases,ref_bases))) # letters to digits\n",
    "\n",
    "    return ref_bases\n",
    "\n",
    "ref_bases = get_ref_bases(variant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "samfile = pysam.AlignmentFile(bamfile, \"rb\" )\n",
    "\n",
    "raw_reads = []\n",
    "\n",
    "EXCLUDE_FLAGS = 0x4|0x200|0x400 #exclude \"segment unmapped\", \"not passing filters\", \"PCR or optical duplicate\" \n",
    "\n",
    "#collect all the reads around the candidate variant position\n",
    "for read in samfile.fetch(variant['chrom'], variant['pos']-2, variant['pos']+2):\n",
    "    if (read.pos<=variant['pos']-1 and read.reference_end>variant['pos'] and \n",
    "        read.flag&EXCLUDE_FLAGS==0):\n",
    "            raw_reads.append((read.pos,read.seq,read.qual,read.flag,read.cigartuples))\n",
    "    if len(raw_reads)>=300:#sometimes there are a lot of reads, we don't need all of them\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phred_qual(qual_symbol):\n",
    "    '''\n",
    "    Return the probability that the base is called incorrectly\n",
    "    \n",
    "    Input: ANCII quality character\n",
    "    Output: Probability\n",
    "    '''\n",
    "    \n",
    "    probability = 10 ** ((ord(qual_symbol) - 33) / -10)\n",
    "    \n",
    "    return probability\n",
    "\n",
    "def compute_VAF_DP(variant_column, alt):\n",
    "    '''\n",
    "    Return VAF and DP for the variant column\n",
    "    '''\n",
    "    DP = len(variant_column)\n",
    "    VAF = (variant_column == encode_bases(alt)).sum()/DP\n",
    "           \n",
    "    return VAF,DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Align reads according to their CIGAR strings.\n",
    "\n",
    "#For each read the cigar string is analysed to place read bases correctly (taking into account clips, insertions,deletion).\n",
    "\n",
    "aligned_reads = []\n",
    "\n",
    "for read_idx, read in enumerate(raw_reads):\n",
    "    \n",
    "    pos,seq,qual,flag,cigartuples = read\n",
    "        \n",
    "    aligned_seq = []  #aligned read sequence\n",
    "    aligned_qual = [] #aligned read qualities\n",
    "    \n",
    "    seq = seq.upper()\n",
    "    seq = encode_bases(seq) # read sequence encoding: letters to digits  \n",
    "    \n",
    "    qual = np.array([get_phred_qual(q) for q in qual])     #probability that the base is called INCORRECTLY\n",
    "    qual = 1.-qual                                              #probability that the base is called CORRECTLY\n",
    "\n",
    "    c = 0 #current position in the original (not aligned) read\n",
    "    \n",
    "    #we move along the original read sequence and make insertions/delections when necessary\n",
    "    for op in cigartuples:\n",
    "        optype, oplen = op #type and length of cigar operation\n",
    "        if optype==5:#hard clip:do nothing as it's not included in seq\n",
    "            continue\n",
    "        elif optype==4:#soft clip: exclude these positions from aligned seq as they aren't used by callers\n",
    "            c+=oplen\n",
    "        elif optype==2 or optype==3 or optype==6: #deletion or padding\n",
    "            aligned_seq.extend([encode_bases('*')]*oplen)\n",
    "            aligned_qual.extend([0]*oplen)\n",
    "        elif optype==1:#insertion\n",
    "            c+=oplen\n",
    "        else: #match or mismatch\n",
    "            aligned_seq.extend(seq[c:c+oplen])\n",
    "            aligned_qual.extend(qual[c:c+oplen])  \n",
    "            c+=oplen\n",
    "            \n",
    "    aligned_reads.append((pos,aligned_seq,aligned_qual,flag))\n",
    "                \n",
    "N_reads = len(aligned_reads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cropping: Read depth (DP): 44, VAF:1.0\n"
     ]
    }
   ],
   "source": [
    "variant_column = np.array([read_seq[(variant['pos']-1)-read_pos] for read_pos, read_seq, *_ in aligned_reads])\n",
    "\n",
    "VAF0,DP0 = compute_VAF_DP(variant_column, variant['alt'])\n",
    "\n",
    "print(f'Before cropping: Read depth (DP): {DP0}, VAF:{VAF0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cropping: Read depth (DP): 20, VAF:1.0\n"
     ]
    }
   ],
   "source": [
    "#if there more reads than we can include in the image, we have to remove some of them\n",
    "\n",
    "if image_crop_strategy == 'center':\n",
    "    #keep reads at the top and at the bottom, remove the central part\n",
    "    aligned_reads = aligned_reads[:image_max_height//2] + aligned_reads[max(N_reads_tot-image_max_height//2,0):N_reads]\n",
    "elif image_crop_strategy == 'topbottom':\n",
    "    #keep reads in the center, remove at the top and at the bottom\n",
    "    shift = max(N_reads//2-image_max_height//2,0)\n",
    "    aligned_reads = aligned_reads[shift:shift+image_max_height]\n",
    "    \n",
    "variant_column = np.array([read_seq[(variant['pos']-1)-read_pos] for read_pos, read_seq, *_ in aligned_reads])\n",
    "\n",
    "VAF,DP = compute_VAF_DP(variant_column, variant['alt'])\n",
    "\n",
    "N_reads = len(aligned_reads)\n",
    "\n",
    "print(f'After cropping: Read depth (DP): {DP}, VAF:{VAF}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "if image_check_variant_column:\n",
    "    #check if (after cropping) we still have the alternative allele in the variant column\n",
    "    variant_column = np.array([read_seq[(variant['pos']-1)-read_pos] for read_pos, read_seq, *_ in aligned_reads])\n",
    "    is_alt = (variant_column==encode_bases(variant['alt']))\n",
    "    if (isinstance(is_alt, bool) and not is_alt) or not is_alt.any():\n",
    "        print('No reads with variant found in the variant column!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "if image_sort_by_variant:\n",
    "    \n",
    "    variant_column = np.array([read_seq[(variant['pos']-1)-read_pos] for read_pos, read_seq, *_ in aligned_reads])\n",
    "    diff = (variant_column==encode_bases(variant['ref'])).astype(int)-(variant_column>3).astype(int).tolist() #all N,M,K letters will go to the bottom\n",
    "    aligned_reads_sorted_tuple = sorted(zip(aligned_reads,diff),key=lambda x:x[1], reverse=True)\n",
    "    aligned_reads = [read[0] for read in aligned_reads_sorted_tuple]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "reads_im = np.zeros((N_reads,image_width,2)) # 2 channels to encode sequence and probability of each read base\n",
    "\n",
    "reads_im[:,:,0] = encode_bases('N') #bases for all reads, default: no data ('N')\n",
    "reads_im[:,:,1] = 1/4. #probability to have the corresponding basis, default:equal probability of all bases\n",
    "\n",
    "for read_idx, read in enumerate(aligned_reads):\n",
    "    \n",
    "    pos,seq,qual,flag = read #absolute position, sequence, quality scores and flags of the read\n",
    "        \n",
    "    start_pos = pos-(variant['pos']-1-variant_column_idx) #relative position of the read in the image\n",
    "    \n",
    "    if start_pos<0: #left end of the read is beyond the defined image edge\n",
    "        \n",
    "        #reject data that's beyond the image\n",
    "        seq = seq[-start_pos:]\n",
    "        qual = qual[-start_pos:]\n",
    "        start_pos = 0\n",
    "        \n",
    "    rlen = len(seq)\n",
    "    \n",
    "    reads_im[read_idx,start_pos:start_pos+rlen,0] = seq[:image_width-start_pos]\n",
    "    reads_im[read_idx,start_pos:start_pos+rlen,1] = qual[:image_width-start_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_bases(variant_new, variant_old, reads_im_old, ref_bases_old):\n",
    "    '''\n",
    "    Replaces sequences for a given variant, while keeping read qualities and flags of original reads\n",
    "    '''\n",
    "    \n",
    "    #variant_column = reads_im_old.shape[2]//2\n",
    "\n",
    "    new_alt = variant_new['alt']\n",
    "    \n",
    "    #get new fragment of reference sequence\n",
    "    new_ref_seq_num = get_ref_bases(variant_new)\n",
    "    new_ref = decode_bases(new_ref_seq_num[variant_column_idx])\n",
    "\n",
    "    #numerical encoding for bases\n",
    "    new_ref_num = new_ref_seq_num[variant_column_idx]\n",
    "    new_alt_num = encode_bases(new_alt)\n",
    "\n",
    "    diff = reads_im_old[:,:,0]-ref_bases_old\n",
    "    diff = np.where(diff==0,1,-1) #indicate whether the current position is diferent from reference bases\n",
    "    diff[reads_im_old[:,:,0]>3] = 0 #special letters (N,K,*, etc): we won't change them\n",
    "\n",
    "    new_ref_seq_num_exp = np.expand_dims(new_ref_seq_num,0) + np.zeros((reads_im_old.shape[0],1)) #expand numerical encoding of reference bases on all reads\n",
    "\n",
    "    new_reads_num = np.zeros((reads_im_old.shape[0],reads_im_old.shape[1])) #numerical encoding of reads bases\n",
    "    \n",
    "    new_reads_num[diff==1] = new_ref_seq_num_exp[diff==1] #copy reference bases to read bases when there's no change\n",
    "    new_reads_num[diff==0] = reads_im[:,:,0][diff==0] #copy special letters\n",
    " \n",
    "    #when there's a difference btw read and reference base, choose any letter which is not the reference base\n",
    "    for i,j in np.vstack(np.where(diff==-1)).T:\n",
    "        choice =[0,1,2,3]\n",
    "        choice.remove(new_ref_seq_num_exp[i,j])\n",
    "        new_reads_num[i,j] = np.random.choice(choice)\n",
    "\n",
    "    #variant column is special: we replace the alternative allele\n",
    "    read_supp_alt = (reads_im_old[:,variant_column_idx,0].astype(int)==encode_bases(variant_old['alt']))\n",
    "    new_reads_num[read_supp_alt, variant_column_idx] = new_alt_num\n",
    "\n",
    "    reads_im_new = np.zeros((reads_im_old.shape))\n",
    "    reads_im_new[:,:,1] = reads_im_old[:,:,1]\n",
    "    reads_im_new[:,:,0] = new_reads_num\n",
    "    \n",
    "    ref_bases_new = new_ref_seq_num\n",
    "    \n",
    "    return reads_im_new, ref_bases_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference sequence: GGTCACAGGGCTGGCCCAGGCTGCAGGAAGCGCGGGAGGTGGACAGGGAGGCTGTGGGTTGCACCCAGGCCCTGGGGCCTCCTCCTGCAGTGATGGGGGCAATGGGTGGCTCTGGGTGTGACCCCAGCTCCGGCCTCCTTTTTCATCTGG\n"
     ]
    }
   ],
   "source": [
    "replace_variant = True #replace variant (data augmentation)\n",
    "\n",
    "if replace_variant:\n",
    "    \n",
    "    new_variant = {k:v for k,v in variant.items()}\n",
    "    \n",
    "    new_variant = {'alt':'T', 'ref':'G', 'chrom':'22', 'refpos':40094296}\n",
    "\n",
    "    reads_im, ref_bases = replace_bases(new_variant, variant, reads_im, ref_bases)\n",
    "    \n",
    "    variant = new_variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCTGTGGGTTGCACCCAGGCCCTGG\u001b[34mG\u001b[0mGCCTCCTCCTGCAGTGATGGGGGC\n",
      "GCTGTGGGTTGCACCCAGGCC\u001b[31mA\u001b[0mTGG\u001b[31mT\u001b[0mGCCTCCTCCTGCAGTGATGGGGNN\n",
      "GCTGTGGGTTGCACCCAGGCC\u001b[31mT\u001b[0mTGG\u001b[31mT\u001b[0mGCCTCCTCCTGCAGTGATGGGGGC\n",
      "GCTGTGGGTTGCACCCAGGCC\u001b[31mT\u001b[0mTGG\u001b[31mT\u001b[0mGCCTCCTCCTGCAGTGATGGGGGC\n",
      "GCTGTGGGTTGCACCCAGGCC\u001b[31mA\u001b[0mTGG\u001b[31mT\u001b[0mGCCTCCTCCTGCAGTGATGGGGGC\n",
      "GCTGTGGGTTGCACCCAGGCC\u001b[31mA\u001b[0mTGG\u001b[31mT\u001b[0mGCCTCCTCCTGCAGTGATGGGGGC\n",
      "GCTGTGGGTTGCACCCAGGCC\u001b[31mA\u001b[0mTGG\u001b[31mT\u001b[0mGCCTCCTCCTGCAGTGATGGGGGC\n",
      "GCTGTGGGTTGCACCCAGGCC\u001b[31mT\u001b[0mTGG\u001b[31mT\u001b[0mGCCTCCTCCTGCAGTGATGGGGGC\n",
      "GCTGTGGGTTGCACCCAGGCC\u001b[31mT\u001b[0mTGG\u001b[31mT\u001b[0mGCCTCCTCCTGCAGTGATGGGGGC\n",
      "GCTGTGGGTTGCACCCAGGCC\u001b[31mG\u001b[0mTGG\u001b[31mT\u001b[0mGCCTCCTCCTGCAGTGATGGGGGC\n",
      "GCTGTGGGTTGCACCCAGGCC\u001b[31mT\u001b[0mTGG\u001b[31mT\u001b[0mGCCTCCTCCTGCAGTGATGGGGGC\n",
      "GCTGTGGGTTGCACCCAGGCC\u001b[31mT\u001b[0mTGG\u001b[31mT\u001b[0mGCCTCCTCCTGCAGTGATGGGGGC\n",
      "GCTGTGGGTTGCACCCAGGCC\u001b[31mG\u001b[0mTGG\u001b[31mT\u001b[0mGCCTCCTCCTGCAGTGATGGGGGC\n",
      "GCTGTGGGTTGCACCCAGGCC\u001b[31mG\u001b[0mTGG\u001b[31mT\u001b[0mGCCTCCTCCTGCAGTGATGGGGGC\n",
      "GCTGTGGGTTGCACCCAGGCC\u001b[31mA\u001b[0mTGG\u001b[31mT\u001b[0mGCCTCCTCCTGCAGTGATGGGGGC\n",
      "GCTGTGGGTTGCACCCAGGCC\u001b[31mG\u001b[0mTGG\u001b[31mT\u001b[0mGCCTCCTCCTGCAGTGATGGGGGC\n",
      "GCTGTGGGTTGCACCCAGGCC\u001b[31mT\u001b[0mTGG\u001b[31mT\u001b[0mGCCTCCTCCTGCAGTGATGGGGGC\n",
      "GCTGTGGGTTGCACCCAGGCC\u001b[31mA\u001b[0mTGG\u001b[31mT\u001b[0mGCCTC\u001b[31mT\u001b[0mTCCTGCAGTGATGGGGGC\n",
      "GCTGTGGGTTGCACCCAGGCC\u001b[31mG\u001b[0mTGG\u001b[31mT\u001b[0mGCCTCCTCCTGCAGTGATGGGGGC\n",
      "GCTGTGGGTTGCACCCAGGCC\u001b[31mG\u001b[0mTGG\u001b[31mT\u001b[0mGCCTCCTCCTGCAGTGATGGGGGC\n",
      "NCTGTGGGTTGCACCCAGGCC\u001b[31mG\u001b[0mTGG\u001b[31mT\u001b[0mGCCTCCTCCTGCAGTGATGGGGGC\n"
     ]
    }
   ],
   "source": [
    "show_diff_text(reads_im, ref_bases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj8AAABcCAYAAACBU9TqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAADWElEQVR4nO3dMW7TYBgGYAchqOjE0IWFIRIbiI0DMHCEXiFHyhUqcYFKnICpohtShy4sReqAxPozIKHGbZ3+xLVjv88z1VGievr19vteN4tSSgMAkOLJ2DcAADAk4QcAiCL8AABRhB8AIIrwAwBEEX4AgChPa978bPG8HDSHj3Uv93rz7vfgvxOoc359tHH99uXVIJ/lft+/vRj7FmBUv5rrn6WUo/brVeHnoDlsPiw+9ndXD3R6ejb47wTqLE9WG9dfj9eDfJb7fXr1fuxbgFF9KZ8v73rd2gsAiFI1+RnLLn+9nP446+0+AKbkf88/EyPmzuQHAIgi/AAAUSax9trFGONbqzZgysY6w6zbGIrJDwAQRfgBAKIIPwBAlNl3fsawbW+tEwRw27azUSeIvpj8AABRhB8AIIrwAwBE0fkZQdfeWh8I4G5d56M+EDVMfgCAKMIPABBF+AEAouj87Jn23loHCGC79lmpA0QXkx8AIIrwAwBEsfbac9ZgAPWswehi8gMARBF+AIAowg8AEEXnZ2Ju7q31fwAe5uZ5qf+DyQ8AEEX4AQCiCD8AQBSdnwmr3VvrCAHUnYX6QfNk8gMARBF+AIAowg8AEEXnJ4j/EQRw2/JktXF9cbz+97PvCJsnkx8AIIrwAwBEsfYK1R7dWoMBqW6uubaxBpsHkx8AIIrwAwBEEX4AgCg6PzRNU7e31g8C+MtXZUyTyQ8AEEX4AQCiCD8AQBSdH6pt21vrBAHc1nU26gMNy+QHAIgi/AAAUYQfACCKzg+922V3rS8EJNrl7NMXqmfyAwBEEX4AgCjCDwAQRfgBAKIIPwBAFOEHAIjiUXf2Sl+PbHpkHkjR53mX8ti8yQ8AEEX4AQCiCD8AQBSdH2apvbfWAcq0PFltXF8cr0e6E5iG9lk51w6QyQ8AEEX4AQCiCD8AQBSdHyJ07a31geZLxwd203U+TrkPZPIDAEQRfgCAKNZexPNYPEC9KT8Wb/IDAEQRfgCAKMIPABBF5wdadIAA6k2pA2TyAwBEEX4AgCjCDwAQZVFKefibF4urpmkuH+92AAB687qUctR+sSr8AABMnbUXABBF+AEAogg/AEAU4QcAiCL8AABRhB8AIIrwAwBEEX4AgCjCDwAQ5Q/p8aDwThPkVQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_diff_image(reads_im, ref_bases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_hot_reads = np.zeros((N_reads,image_width,4)) # p-hot encoding of reads probabilities: each channel gives the probability of the corresponding base (ACTG)\n",
    "\n",
    "for basis_idx in range(4):\n",
    "    p_hot_reads[:,:,basis_idx] = np.where(reads_im[:,:,0]==basis_idx, reads_im[:,:,1], (1.-reads_im[:,:,1])/3.)\n",
    "\n",
    "del_row, del_col = np.where(reads_im[:,0,:]==6) #deletions\n",
    "p_hot_reads[del_row, del_col, :] = 0.\n",
    "\n",
    "M_row, M_col = np.where(reads_im[:,0,:]==7) #M: either A or C, each with probability 0.5\n",
    "p_hot_reads[M_row, M_col, 0] = 1.\n",
    "p_hot_reads[M_row, M_col, 1] = 1.\n",
    "\n",
    "K_row, K_col = np.where(reads_im[:,0,:]==5) #K: either G or T, each with probability 0.5\n",
    "p_hot_reads[K_row, K_col, 2] = 1.\n",
    "p_hot_reads[K_row, K_col, 3] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_ref = np.zeros((1,image_width,4)) #one-hot encoding of reference bases, same for all reads\n",
    "\n",
    "for basis_idx in range(4):\n",
    "    one_hot_ref[:,(ref_bases==basis_idx),basis_idx] = 1.\n",
    "\n",
    "one_hot_ref[:,(ref_bases==4),:] = 1 #N\n",
    "one_hot_ref[:,(ref_bases==7),0] = 1 #M\n",
    "one_hot_ref[:,(ref_bases==7),1] = 1 #M\n",
    "one_hot_ref[:,(ref_bases==5),2] = 1 #K\n",
    "one_hot_ref[:,(ref_bases==5),3] = 1 #K\n",
    "    \n",
    "flags_reads = np.zeros((N_reads,image_width,6)) # encoding of 6 flags, different for all reads\n",
    "\n",
    "#loop over flags of all reads\n",
    "for read_idx, (_,_,_,flag) in enumerate(aligned_reads):\n",
    "    flags_reads[read_idx,:,0]=flag&0x2   #each segment properly aligned according to the aligner\n",
    "    flags_reads[read_idx,:,1]=flag&0x8   #next segment unmapped\n",
    "    flags_reads[read_idx,:,2]=flag&0x10  #SEQ being reverse complemented\n",
    "    flags_reads[read_idx,:,3]=flag&0x20  #SEQ of the next segment in the template being reverse complemented\n",
    "    flags_reads[read_idx,:,4]=flag&0x100 #secondary alignment\n",
    "    flags_reads[read_idx,:,5]=flag&0x800 #supplementary alignment\n",
    "    flags_reads[read_idx,:] = flags_reads[read_idx,:]>0 #to boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualize reference channels (they are the same for all reads)\n",
    "\n",
    "N_crop = 50 #bases to omit on each side of the variant position, to make lines shorter\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "nucl_colors=np.array([(255,255,255), (228,26,28), (55,126,184), (77,175,74), (152,78,163)])/255. #colors for NACTG\n",
    "nucl_cmap = ListedColormap(nucl_colors)\n",
    "\n",
    "ref_im = np.transpose(one_hot_ref[0, N_crop:-N_crop,:]).copy().astype(np.ubyte) #reference tensor is the same for all reads, we choose read 0\n",
    "\n",
    "for base_idx in range(4):\n",
    "    ref_im[base_idx,:]=ref_im[base_idx,:]*(base_idx+1) #color corresponds to the base index  \n",
    "\n",
    "ref_letters = decode_bases(ref_bases) #seqence decoding:map digits back to letters\n",
    "\n",
    "pcolor(ref_im, yticklabels=['A','C','T','G'], xticklabels=ref_letters[N_crop:-N_crop], cmap=nucl_cmap, figsize = (15,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualize channels specific to a given read\n",
    "\n",
    "read_number = 1\n",
    "\n",
    "N_crop = 50 #bases to omit on each side of the variant position, to make lines shorter\n",
    "\n",
    "reads_letters = [list(map(lambda x:decode_bases(x), x)) for x in reads_im[:,0,:]] #seqence decoding:map digits back to letters\n",
    "\n",
    "phreads_im = np.transpose(p_hot_reads[read_number,N_crop:-N_crop,:]).copy() \n",
    "pcolor(phreads_im, yticklabels=['A','C','T','G'], xticklabels=reads_letters[read_number][N_crop:-N_crop], cmap='Greens',figsize = (15,15))\n",
    "\n",
    "flags_im = np.transpose(flags_reads[read_number,N_crop:-N_crop,:])\n",
    "pcolor(flags_im, yticklabels=['0x2','0x8','0x10','0x20','0x100', '0x800'], xticklabels=reads_letters[read_number][N_crop:-N_crop], cmap='binary',figsize = (15,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flag: 147\n",
      "\n",
      "Flag    1 (0x1  ): True\n",
      "Flag    2 (0x2  ): True\n",
      "Flag    4 (0x4  ): False\n",
      "Flag    8 (0x8  ): False\n",
      "Flag   16 (0x10 ): True\n",
      "Flag   32 (0x20 ): False\n",
      "Flag   64 (0x40 ): False\n",
      "Flag  128 (0x80 ): True\n",
      "Flag  256 (0x100): False\n",
      "Flag  512 (0x200): False\n",
      "Flag 1024 (0x400): False\n",
      "Flag 2048 (0x800): False\n"
     ]
    }
   ],
   "source": [
    "#print values of all flags\n",
    "all_reads_flags = [flag for (_,_,_,flag) in aligned_reads]\n",
    "print(f'Flag: {all_reads_flags[read_number]}\\n')\n",
    "for flag in map(lambda x: 2**x,range(12)):\n",
    "    print(f'Flag {flag:4d} ({hex(flag):5}): {all_reads_flags[read_number]&flag>0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--. 1 sergey.vilov OG-ICB-User 42K Dec 16  2021 test.img\n"
     ]
    }
   ],
   "source": [
    "# How much memory does one image take?\n",
    "\n",
    "image = {'one_hot_ref':one_hot_ref.astype(bool),\n",
    "             'p_hot_reads':(p_hot_reads*1e4).astype(np.ushort),  \n",
    "            'flags_reads':flags_reads.astype(bool)}\n",
    "    \n",
    "import pickle\n",
    "\n",
    "with open('test.img', 'wb') as f:\n",
    "    pickle.dump(image,f)\n",
    "    \n",
    "!ls -alh test.img\n",
    "!rm test.img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CAGGCCCTGGGGCCTCCTCCT'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_support = ''.join(decode_bases(ref_bases)[variant_column_idx-10:variant_column_idx+11]) # reference sequence around the variant\n",
    "ref_support"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
