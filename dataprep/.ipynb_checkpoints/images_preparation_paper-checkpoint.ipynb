{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create variant tensors\n",
    "\n",
    "We construct variant tensors for each variant in a given VCF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pysam #library for reading VCF files\n",
    "\n",
    "sys.path.append(\"python/\")\n",
    "\n",
    "from variant_to_tensor import variant_to_tensor #function to form a tensor out of a variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dotdict(dict):\n",
    "    \"\"\"\n",
    "    Dictionary with dot.notation access to attributes\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_batch(batch, info, batch_path): \n",
    "    '''\n",
    "    Write a batch of tensors on the disk\n",
    "    '''\n",
    "    #print(batch_path)\n",
    "    \n",
    "    if not SIMULATE:\n",
    "        with open(batch_path, 'wb') as f:\n",
    "            pickle.dump({'images':batch, 'info':info},f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_tensors(vcf :str,                             #full path to a VCF file with the variants\n",
    "               bam_dir: str,                          #directory with corresponding BAM files\n",
    "               output_dir :str,                       #output dir for tensor batches\n",
    "               refgen_fa :str,                        #reference genome FASTA file\n",
    "               tensor_opts :Dict,                     #options for variant tensor encoding\n",
    "               Lbatch :Optional[int] = 1,             #how many tensors put in each batch\n",
    "               chrom :Optional[str] = None,           #chromosome name\n",
    "               max_variants :Optional[int] = None,    #stop when this number of variants is reached\n",
    "               bam_matching_csv :Optional[str] = '',  #matching table between BAM sample name and BAM file name\n",
    "             ):         \n",
    "    '''\n",
    "    Create a pileup tensor for each variant in the given VCF file.\n",
    "    \n",
    "    For each variant a sample BAM file is required.\n",
    "    BAM file name can be encoded directly as a record BAM=bam_file_name.bam in the VCF INFO field (without the path).\n",
    "    Otherwise, it is inferred from the sample name in the VCF file using bam_matching_csv.\n",
    "    \n",
    "    Tensors a packed in batches of size Lbatch.\n",
    "    Depending on the global SIMULATE value the batches are saved to the disk.\n",
    "    To avoid file system issues, we distribute batches into subfolders in the output_dir.\n",
    "\n",
    "    To keep record of variants created, variant annotations (DP, VAF etc...) are added to the variants_df dataframe.\n",
    "    To speed up processing, they are first accumulated in variants_list and added to the variants_df only when 1000 variants are accumulated.\n",
    "    \n",
    "    Tensor options (width, height etc...) are defined in the tensor_opts dictionary.\n",
    "    See the variant_to_tensor function to learn more about tensor options.\n",
    "    '''\n",
    "    \n",
    "    tensors_per_subdir = 100*Lbatch #maximum tensors per subdir\n",
    "    \n",
    "    variants_df = pd.DataFrame(columns=[\"vcf\", \"vcf_record_idx\", \"chrom\", \"pos\", \"ref\", \"alt\", \"ref_support\", \"BAM\", \"VAF\", \"DP\", \"tensor_height\", \"batch_name\", \"subdir\"]) # DataFrame for variant annotations \n",
    "    \n",
    "    if not SIMULATE:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "    if bam_matching_csv:\n",
    "        #matching table between BAM sample name and BAM file name\n",
    "        #otherwise, the INFO filed of the VCF file should have the BAM=bam_file_name.bam record \n",
    "        bam_matching = pd.read_csv(bam_matching_csv, names=['BAM_sample', 'BAM_file'], squeeze=True, index_col=0)\n",
    "        bam_matching = bam_matching.apply(lambda x:x.replace('.bam','')+'.bam')\n",
    "\n",
    "    vcf_in = pysam.VariantFile(vcf) #open the VCF file\n",
    "    \n",
    "    all_samples = list(vcf_in.header.samples) #extract BAM sample names from the VCF header\n",
    "                    \n",
    "    variants_batch = [] #current batch of tensors\n",
    "        \n",
    "    variants_list = []  #we will first accumulate variant annotations in a list and then add this list to the data frame\n",
    "    \n",
    "    N_variants_added = 0 #total number of variants added\n",
    "        \n",
    "    #iterate over the records of the vcf file\n",
    "    for vcf_record_idx, rec in enumerate(iter(vcf_in.fetch(contig = chrom))):\n",
    "        \n",
    "        if vcf_record_idx%tensors_per_subdir==0:\n",
    "            #switch to a new subdir if the current one already has enough batches\n",
    "            batch_subdir = str(vcf_record_idx//tensors_per_subdir)\n",
    "            os.makedirs(os.path.join(output_dir, batch_subdir), exist_ok = True)\n",
    "                        \n",
    "        if max_variants and N_variants_added >= max_variants:\n",
    "            break\n",
    "            \n",
    "        #in a VCF file we have BAM sample names and we need the names of corresponding BAM files        \n",
    "        if bam_matching_csv:\n",
    "            #get the file name from the matching table\n",
    "            bam_sample_names = [s for s in all_samples if rec.samples[s]['GT']!=(None,None)]\n",
    "            bam_file_names = bam_matching.loc[bam_sample_names]\n",
    "        else:\n",
    "            #otherwise, use BAM file name from the VCF record\n",
    "            bam_file_name = rec.info.get('BAM')[0].replace('.bam','')+'.bam' #when the BAM file name is defined in the INFO field\n",
    "            bam_file_names = [bam_file_name] #for compatibility\n",
    "\n",
    "\n",
    "        #loop over all BAM files that have this variant\n",
    "        for bam_file_name in bam_file_names:\n",
    " \n",
    "                bam_path = os.path.join(bam_dir, bam_file_name) #full path to the BAM file\n",
    "                \n",
    "                variant = {'pos':rec.pos, 'refpos':rec.pos, 'chrom':rec.chrom, 'ref':rec.ref, 'alt':rec.alts[0]}\n",
    "\n",
    "                try:\n",
    "                    \n",
    "                    #get a tensor variant tensor for the current variant\n",
    "                    variant_tensor, ref_support, VAF, DP = variant_to_tensor(variant, refgen_fa, bam_path, check_variant=\"snps\",\n",
    "                         **tensor_opts) \n",
    "\n",
    "                except Exception as exc:\n",
    "                    \n",
    "                    print('-------------------------------------------------')\n",
    "                    print('Exception occured while creating a variant tensor')\n",
    "                    print('Variant:\\n', variant)\n",
    "                    print('Reference FASTA file:\\n', refgen_fa)\n",
    "                    print('BAM file:\\n', bam_path)\n",
    "                    print('Error message:\\n', exc)\n",
    "\n",
    "                    continue\n",
    "\n",
    "                variants_batch.append(variant_tensor) #add current variant to the batch\n",
    "                                                \n",
    "                variant_record = {\n",
    "                     'vcf_record_idx':vcf_record_idx,\n",
    "                     'subdir': batch_subdir,\n",
    "                     'BAM': bam_file_name,\n",
    "                     'VAF': VAF,\n",
    "                     'DP': DP,\n",
    "                     'tensor_height':variant_tensor['p_hot_reads'].shape[0],\n",
    "                     'ref_support': ref_support,\n",
    "                    }\n",
    "                                \n",
    "                variants_list.append(variant_record)\n",
    "\n",
    "                N_variants_added += 1\n",
    "                \n",
    "                if N_variants_added%Lbatch == 0:\n",
    "                    \n",
    "                    #save the batch to the disk when it is full\n",
    "                                  \n",
    "                    batch_name = f'{variants_list[-Lbatch][\"vcf_record_idx\"]}.imgb' #batch name: VCF record index of the 1st variant in the batch\n",
    "                    \n",
    "                    for i in range(-Lbatch,0):\n",
    "                        variants_list[i]['batch_name']=batch_name #mark batch name in the variants list\n",
    "    \n",
    "                    if not SIMULATE:\n",
    "                        #save batch to the disk\n",
    "                        dump_batch(variants_batch, variants_list[-Lbatch:], os.path.join(*[output_dir, batch_subdir, batch_name]))\n",
    "                \n",
    "                    variants_batch = [] #empty current batch\n",
    "   \n",
    "                    if  len(variants_list)>1000:\n",
    "                        #add variants_list to variants_df every 1000 tensors\n",
    "                        variants_df = variants_df.append(variants_list, ignore_index=True)\n",
    "                        variants_list = []    \n",
    "\n",
    "    N_batch = len(variants_batch)\n",
    "        \n",
    "    if N_batch:\n",
    "                \n",
    "        batch_name = f'{variants_list[-Lbatch][\"batch_name\"]}.imgb' #batch name: VCF record index of the 1st variant in the batch\n",
    "        \n",
    "        for i in range(-N_batch,0):\n",
    "            variants_list[i]['batch_name']=batch_name #mark batch name in the variants list\n",
    "                    \n",
    "        if not SIMULATE:\n",
    "            #save batch to the disk\n",
    "            dump_batch(variants_batch, variants_list[-N_batch:], os.path.join(*[output_dir,  batch_subdir, batch_name]))\n",
    "    \n",
    "    variants_df = variants_df.append(variants_list, ignore_index=True)\n",
    "\n",
    "    return variants_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_params = dotdict({})\n",
    "\n",
    "input_params.vcf = '/storage/groups/epigenereg01/workspace/projects/vale/calling/MLL/germline/filtered/vqsr/99.9/t.vcf.gz'#'/storage/groups/epigenereg01/workspace/projects/vale/datasets/snvs/GACA-CN/gnomAD_thr_0/vcfs/negative_train_nn.vcf.gz' #vcf file with variants\n",
    "input_params.output_dir = '/storage/groups/epigenereg01/workspace/projects/vale/datasets/snvs/test'#'/storage/groups/epigenereg01/workspace/projects/vale/datasets/snvs/GACA-CN/gnomAD_thr_0/images/' #output dir name\n",
    "input_params.bam_dir = '/storage/groups/epigenereg01/datasets/MLL-5000-genomes/matched_pairs/BAM/'#'/storage/groups/epigenereg01/workspace/projects/vale/data/icgc/GACA-CN/bam/' #folder with BAM files\n",
    "input_params.refgen_fa = '/storage/groups/epigenereg01/workspace/projects/vale/calling/MLL/resources_GRCh37/GRCh37.fa' #Reference genome FASTA file\n",
    "input_params.Lbatch = 1#4 #size of tensors batches\n",
    "input_params.chrom = None#'1' #chromosome name, to limit tensors generation to a particular contig\n",
    "input_params.bam_matching_csv = '/storage/groups/epigenereg01/datasets/MLL-5000-genomes/matched_pairs/BAM/bam_matching.csv' #matching table between BAM sample name and BAM file name (see gen_tensors)\n",
    "input_params.max_variants = None #maximum number of variants from the VCF to consider\n",
    "input_params.tensor_width = 150 # tensor width: 2x the most probable read length\n",
    "input_params.tensor_max_height = 50 #max tensor height, the probability to have a read depth above this value should be small\n",
    "input_params.tensor_crop_strategy = 'topbottom' #how to crop variant tensor when read depth>tensor_max_height\n",
    "input_params.tensor_sort_by_variant = True #sort reads by base in the variant column\n",
    "input_params.tensor_check_variant_column = True #check if the variant is present in actual pileup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMULATE = 0 #simulate, don't create any folders or wite any tensors to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SIMULATE:\n",
    "    os.makedirs(input_params.output_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Exception occured while creating a variant tensor\n",
      "Variant:\n",
      " {'pos': 10583, 'refpos': 10583, 'chrom': '1', 'ref': 'G', 'alt': 'A'}\n",
      "Reference FASTA file:\n",
      " /storage/groups/epigenereg01/workspace/projects/vale/calling/MLL/resources_GRCh37/GRCh37.fa\n",
      "BAM file:\n",
      " /storage/groups/epigenereg01/datasets/MLL-5000-genomes/matched_pairs/BAM/p_0_55.tumor.bam\n",
      "Error message:\n",
      " '>' not supported between instances of 'NoneType' and 'int'\n",
      "-------------------------------------------------\n",
      "Exception occured while creating a variant tensor\n",
      "Variant:\n",
      " {'pos': 66355, 'refpos': 66355, 'chrom': '1', 'ref': 'A', 'alt': 'T'}\n",
      "Reference FASTA file:\n",
      " /storage/groups/epigenereg01/workspace/projects/vale/calling/MLL/resources_GRCh37/GRCh37.fa\n",
      "BAM file:\n",
      " /storage/groups/epigenereg01/datasets/MLL-5000-genomes/matched_pairs/BAM/p_0_2.tumor.bam\n",
      "Error message:\n",
      " No reads with the alternative allele found in the variant column!\n",
      "-------------------------------------------------\n",
      "Exception occured while creating a variant tensor\n",
      "Variant:\n",
      " {'pos': 66355, 'refpos': 66355, 'chrom': '1', 'ref': 'A', 'alt': 'T'}\n",
      "Reference FASTA file:\n",
      " /storage/groups/epigenereg01/workspace/projects/vale/calling/MLL/resources_GRCh37/GRCh37.fa\n",
      "BAM file:\n",
      " /storage/groups/epigenereg01/datasets/MLL-5000-genomes/matched_pairs/BAM/p_0_75.tumor.bam\n",
      "Error message:\n",
      " No reads with the alternative allele found in the variant column!\n",
      "-------------------------------------------------\n",
      "Exception occured while creating a variant tensor\n",
      "Variant:\n",
      " {'pos': 88710, 'refpos': 88710, 'chrom': '1', 'ref': 'C', 'alt': 'G'}\n",
      "Reference FASTA file:\n",
      " /storage/groups/epigenereg01/workspace/projects/vale/calling/MLL/resources_GRCh37/GRCh37.fa\n",
      "BAM file:\n",
      " /storage/groups/epigenereg01/datasets/MLL-5000-genomes/matched_pairs/BAM/p_0_75.tumor.bam\n",
      "Error message:\n",
      " '>' not supported between instances of 'NoneType' and 'int'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6984/2650997469.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mvariants_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_opts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_opts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgen_params\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#dataframe with annotations of processed variants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mvcf_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvcf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.vcf.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#VCF base name without extentension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6984/1526204670.py\u001b[0m in \u001b[0;36mgen_tensors\u001b[0;34m(vcf, bam_dir, output_dir, refgen_fa, tensor_opts, Lbatch, chrom, max_variants, bam_matching_csv)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0;31m#get a tensor variant tensor for the current variant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     variant_tensor, ref_support, VAF, DP = variant_to_tensor(variant, refgen_fa, bam_path, check_variant=\"snps\",\n\u001b[0m\u001b[1;32m     85\u001b[0m                          **tensor_opts) \n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/home/icb/sergey.vilov/workspace/vale-variant-calling/neural_network/dataset_preparation/images_prep/paper/python/variant_to_tensor.py\u001b[0m in \u001b[0;36mvariant_to_tensor\u001b[0;34m(variant, ref_fasta_file, bam_file, tensor_width, tensor_height, tensor_sort_by_variant, tensor_check_variant_column, tensor_crop_strategy, check_variant)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0mref_bases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ref_bases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariant\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0msamfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpysam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAlignmentFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbam_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;31m#open the variant BAM file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0mraw_reads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tensor_opts = dict() #parameters for the variant_to_tensor function\n",
    "\n",
    "gen_params = dict() #parameters for the gen_tensors function\n",
    "\n",
    "for param,value in input_params.items():\n",
    "    #from input parameters, separate parameters for gen_tensors and variant_to_tensor functions\n",
    "    if not param.startswith('tensor_'):\n",
    "        gen_params[param] = value\n",
    "    else:\n",
    "        tensor_opts[param] = value\n",
    "        \n",
    "if gen_params['chrom'] != None:\n",
    "    #if we are limited to a particular contig, put generated tensors in a dedicated folder\n",
    "    gen_params['output_dir'] = os.path.join(gen_params['output_dir'], gen_params['chrom'])\n",
    "    \n",
    "t0 = time.time()\n",
    "\n",
    "variants_df = gen_tensors(tensor_opts = tensor_opts, **gen_params) #dataframe with annotations of processed variants\n",
    "\n",
    "vcf_name = os.path.basename(input_params.vcf).replace('.vcf.gz', '') #VCF base name without extentension\n",
    "\n",
    "variants_df['vcf'] = vcf_name\n",
    "\n",
    "variants_df.to_csv(os.path.join(gen_params['output_dir'], \"variants.csv.gz\"))\n",
    "\n",
    "t_exec = time.time() - t0 #total execution time\n",
    "\n",
    "print(f\"{gen_params['output_dir']}\\nFinished successfully. Execution time: {t_exec//60:.0f}m {t_exec%60:.1f}s.\")\n",
    "print(f'{len(variants_df)} variants is created, distributed over {len(variants_df.batch_name.unique())} batches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
